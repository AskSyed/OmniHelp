# RAG Application

**Retrieval-Augmented Generation Application with Langgraph, FastAPI, and ChromaDB**

A production-ready RAG (Retrieval-Augmented Generation) application that supports PDF and CSV document ingestion, intelligent query processing, and multi-step agent workflows using Langgraph.

## ğŸ—ï¸ Architecture

The application uses Langgraph to orchestrate a sophisticated multi-step RAG workflow:

1. **Query Classification**: Analyzes user queries and determines intent
2. **Context Retrieval**: Performs vector similarity search in ChromaDB
3. **Answer Generation**: Uses LLM to generate answers from retrieved context
4. **Answer Refinement**: Refines and validates the generated response

```mermaid
graph TD
    A[User Query] --> B[Query Agent]
    B --> C[Retrieval Agent]
    C --> D[ChromaDB Vector Search]
    D --> E[Generation Agent]
    E --> F[Refinement Agent]
    F --> G[Final Answer]
    
    H[PDF Upload] --> I[PDF Parser]
    J[CSV Upload] --> K[CSV Parser]
    I --> L[Text Chunker]
    K --> L
    L --> M[Embedding Generator]
    M --> D
```

## ğŸ› ï¸ Tech Stack

- **Python 3.11+**: Latest Python features
- **FastAPI**: Modern, fast web framework
- **Langgraph**: Agent orchestration and workflow management
- **LangChain**: LLM integration and RAG capabilities
- **ChromaDB**: Vector database for embeddings
- **OpenAI**: LLM and embeddings (GPT-4, text-embedding-3-small)

## ğŸ“ Project Structure

```
OmniHelp/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/              # Langgraph agents
â”‚   â”‚   â”‚   â”œâ”€â”€ query_agent.py   # Query classification
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval_agent.py # Vector search
â”‚   â”‚   â”‚   â”œâ”€â”€ generation_agent.py # Answer generation
â”‚   â”‚   â”‚   â”œâ”€â”€ refinement_agent.py # Answer refinement
â”‚   â”‚   â”‚   â””â”€â”€ orchestrator.py  # Main workflow
â”‚   â”‚   â”œâ”€â”€ api/                 # FastAPI routes
â”‚   â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚   â”‚       â”œâ”€â”€ documents.py # Document upload endpoints
â”‚   â”‚   â”‚       â””â”€â”€ query.py     # Query endpoints
â”‚   â”‚   â”œâ”€â”€ core/                # Configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py        # Settings
â”‚   â”‚   â”‚   â””â”€â”€ dependencies.py  # Shared dependencies
â”‚   â”‚   â”œâ”€â”€ db/                  # Database layer
â”‚   â”‚   â”‚   â””â”€â”€ chroma.py        # ChromaDB client
â”‚   â”‚   â”œâ”€â”€ models/              # Data models
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”‚   â””â”€â”€ query.py
â”‚   â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ document_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking_service.py
â”‚   â”‚   â”‚   â””â”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ utils/               # Utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”‚   â””â”€â”€ parsers.py
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .gitignore
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_services.py
â”‚   â””â”€â”€ test_api.py
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11 or higher
- OpenAI API key
- pip (Python package manager)

### Installation

1. **Clone or navigate to the project directory:**
   ```bash
   cd OmniHelp/backend
   ```

2. **Create virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**
   
   Create a `.env` file in the `backend/` directory:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   OPENAI_MODEL=gpt-4o-mini
   OPENAI_EMBEDDING_MODEL=text-embedding-3-small
   CHROMA_DB_PATH=./data/chroma_db
   CHUNK_SIZE=1000
   CHUNK_OVERLAP=200
   ```

5. **Create data directories:**
   ```bash
   mkdir -p data/documents data/chroma_db logs
   ```

6. **Run the application:**
   ```bash
   python -m app.main
   ```
   
   Or using uvicorn directly:
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

   The API will be available at `http://localhost:8000`
   - API Documentation: `http://localhost:8000/docs`
   - ReDoc: `http://localhost:8000/redoc`

## ğŸ“– Usage

### 1. Upload Documents

**Upload a PDF:**
```bash
curl -X POST "http://localhost:8000/api/v1/documents/upload" \
  -F "file=@document.pdf"
```

**Upload a CSV:**
```bash
curl -X POST "http://localhost:8000/api/v1/documents/upload" \
  -F "file=@data.csv"
```

**Response:**
```json
{
  "document_id": "uuid-here",
  "filename": "document.pdf",
  "document_type": "pdf",
  "chunks": 45,
  "file_path": "./data/documents/uuid_document.pdf"
}
```

### 2. Query Documents

**Submit a query:**
```bash
curl -X POST "http://localhost:8000/api/v1/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is machine learning?",
    "n_results": 5
  }'
```

**Response:**
```json
{
  "answer": "Machine learning is...",
  "sources": ["document.pdf", "another.pdf"],
  "retrieved_chunks": [...],
  "metadata": {
    "refined": true,
    "sources_count": 2
  }
}
```

### 3. List Documents

```bash
curl -X GET "http://localhost:8000/api/v1/documents/"
```

### 4. Delete Document

```bash
curl -X DELETE "http://localhost:8000/api/v1/documents/{document_id}"
```

## ğŸ”„ How It Works

### Document Processing Flow

1. **Upload**: PDF or CSV file is uploaded via API
2. **Parsing**: 
   - PDF: Text extraction using pypdf
   - CSV: Each row converted to text with column context
3. **Chunking**: Text split into chunks (configurable size/overlap)
4. **Embedding**: Generate embeddings using OpenAI
5. **Storage**: Store chunks with metadata in ChromaDB

### Query Processing Flow

1. **Query Classification**: Analyze intent and determine search strategy
2. **Retrieval**: Vector similarity search in ChromaDB
3. **Generation**: LLM generates answer from retrieved context
4. **Refinement**: Answer is refined and validated
5. **Response**: Final answer returned with sources and metadata

## ğŸ§ª Testing

Run tests:
```bash
pytest tests/
```

Run with coverage:
```bash
pytest tests/ --cov=app --cov-report=html
```

## âš™ï¸ Configuration

Key configuration options in `.env`:

- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `OPENAI_MODEL`: LLM model (default: gpt-4o-mini)
- `OPENAI_EMBEDDING_MODEL`: Embedding model (default: text-embedding-3-small)
- `CHROMA_DB_PATH`: Path to ChromaDB storage
- `CHUNK_SIZE`: Text chunk size (default: 1000)
- `CHUNK_OVERLAP`: Chunk overlap (default: 200)
- `MAX_FILE_SIZE_MB`: Maximum upload size (default: 50MB)

## ğŸ“ API Endpoints

### Documents
- `POST /api/v1/documents/upload` - Upload PDF or CSV
- `GET /api/v1/documents/` - List all documents
- `DELETE /api/v1/documents/{document_id}` - Delete document

### Query
- `POST /api/v1/query` - Submit RAG query

### Health
- `GET /health` - Health check
- `GET /` - Root endpoint

## ğŸš§ Future Enhancements

- [ ] Support for additional file formats (DOCX, TXT, etc.)
- [ ] Advanced metadata filtering
- [ ] Query history and conversation context
- [ ] Multi-document query support
- [ ] Custom embedding models
- [ ] Document versioning
- [ ] User authentication
- [ ] Rate limiting
- [ ] Caching layer

## ğŸ“„ License

This project is part of an AI engineering demonstration.

## ğŸ¤ Contributing

For production use, consider:
- Adding comprehensive error handling
- Implementing rate limiting
- Adding authentication/authorization
- Setting up proper logging and monitoring
- Adding more comprehensive tests
- Implementing caching strategies
- Adding document versioning

---

**Built with â¤ï¸ using Langgraph, FastAPI, ChromaDB, and OpenAI**
